{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "master = \"https://raw.githubusercontent.com/mcelikkaya/medium_articles/main/japan_wiki.txt\"\n",
    "req = requests.get(master)\n",
    "req = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = req.split(\"\\n\")\n",
    "all_sentences = [s.replace(\"\\r\",\"\") for s in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size :  40389\n",
      "samples     : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hokkaido was formerly known as Ezo  Yezo  Yeso  or Yesso.',\n",
       " 'According to Matsuura  the name was thought up because the Ainu called the region Kai.',\n",
       " 'In contrast to the island of Honshu  Hokkaido saw an absence of conflict during this time period.',\n",
       " 'From the Middle Ages  the people in Hokkaido began to be called Ezo.',\n",
       " 'Hokkaido subsequently became known as Ezochi  蝦夷地  lit.',\n",
       " 'The disputes eventually developed into war.',\n",
       " 'Takeda Nobuhiro killed the Ainu leader  Koshamain  and defeated the opposition in 1457.',\n",
       " 'The Matsumae family s economy relied upon trade with the Ainu.',\n",
       " 'They held authority over the south of Ezochi until the end of the Edo period.',\n",
       " 'There were numerous revolts by the Ainu against the feudal rule.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"sample size : \",len(all_sentences))\n",
    "print(\"samples     : \" )\n",
    "all_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16504, 11790]\n",
      "[16504]\n",
      "[73, 2674, 284, 2584, 78]\n",
      "[73, 2674]\n",
      "[83, 482, 8226]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "#get pretrained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='', eos_token='', pad_token='')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#tokenizer some samples\n",
    "print( tokenizer.encode(\"Japan Tokyo\") )\n",
    "print( tokenizer.encode(\"Japan\") )\n",
    "print( tokenizer.encode(\"japan tokyo\") )\n",
    "print( tokenizer.encode(\"japan\") )\n",
    "print( tokenizer.encode(\"tokyo\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(tokenizer.encode(s)) for s in all_sentences])\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we will be feeding with sentences from wikipedia\n",
    "#we can mark beginning and end of sentences with with sos and eos\n",
    "def tokenize_seq(sent,tokenizer,max_length):\n",
    "  return tokenizer(''+ sent + '', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "class JapanDataset(Dataset):\n",
    "\n",
    "  def __init__(self, sentences, tokenizer, gpt2_type=\"gpt2\", max_length=max_len):\n",
    "\n",
    "    self.tokenizer = tokenizer \n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for sentence in sentences:      \n",
    "      encodings = tokenize_seq(sentence,tokenizer,max_length)\n",
    "            \n",
    "      self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]   \n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed))))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size : 36350\n",
      "val_size   : 4039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of Dataset\n",
    "dataset = JapanDataset(all_sentences, tokenizer, max_length=max_len)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "print(\"train_size :\",train_size)\n",
    "print(\"val_size   :\",val_size)\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   39,   482,    74, 44354,   373, 15734,  1900,   355,   412, 10872,\n",
       "           220,   575,  8471,    78,   220,  3363,    78,   220,   393,   575,\n",
       "           408,    78,    13, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check a sample from dataset \n",
    "#50257 beginning of sentence token\n",
    "#50258 end of sentence token\n",
    "#50259 pad token\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataloaders\n",
    "train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set), batch_size=16)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "configuration = GPT2Config.from_pretrained(\"gpt2\", output_hidden_states=False)\n",
    "# Load pretrained gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create device\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0005)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at every step i want to check if generations are getting better.\n",
    "def eval_keywords(keywords):\n",
    "    model.eval()\n",
    "    for keyword in keywords:\n",
    "        input_seq = \" \" + keyword\n",
    "        generated = torch.tensor(tokenizer.encode(input_seq)).unsqueeze(0)\n",
    "        generated = generated.to(device)\n",
    "        sample_outputs = model.generate(generated, do_sample=True, top_k=30, max_length=50, top_p=0.9, num_return_sequences=2)\n",
    "\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "keywords = [\"Osaka\",\"Japan\",\"Kyoto\",\"Yokohama\",\"Kanto\",\"Nikko\",\"Japan has\",\"Tokyo is the\",\"Osaka is the\",\"Kyoto is the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call model with a batch of input\n",
    "def process_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    outputs = model(b_input_ids, attention_mask = b_masks, labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#do one epoch for training\n",
    "def train_epoch():\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = process_one_batch(batch)\n",
    "        loss = outputs[0]\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss+=batch_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    print(\"avg_train_loss\",avg_train_loss)  \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 training epoch : \",elapsed_time)\n",
    "\n",
    "#do one epoch for eval\n",
    "def eval_epoch():\n",
    "    t0 = time.time()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = process_one_batch( batch)\n",
    "            loss = outputs[0]              \n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss         \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"avg_val_loss\",avg_val_loss) \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 eval epoch : \",elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss 0.8565802537954189\n",
      "elapsed time for 1 training epoch :  0:08:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss 0.7495614122967476\n",
      "elapsed time for 1 eval epoch :  0:00:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Osakayo has a population of around 1.12 million  2018.\n",
      "1:  Osaka has been ranked 9th in the world in quality of food  food  and waste water.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Japan has the most female male births with over 1.1 billion babies.\n",
      "1:  Japan is also an important trading partner of China.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Kyoto       Tokyo has two major highways  a one way traffic and a three way traffic lane.\n",
      "1:  Kyoto  Japan is a major exporter of food  fish  vegetables  and meat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Yokohama is the capital of the prefecture.\n",
      "1:  Yokohama was also a major transportation port for Japan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Kanto and Ōsaka were the three largest cities of Japan at 1 479 km.\n",
      "1:  Kanto  匈  the center of the koto dance  is one of the most popular  and has become popular in Japan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Nikko was born on April 12  1943  in Tokyo.\n",
      "1:  Nikko      In a referendum on 23 December 1991  the President was replaced by a constitutional monarch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Japan has the second highest average household wealth in the world at  0.5.\n",
      "1:  Japan has a low cost of living  with over 11 000 homes and more than 2 000 shops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Tokyo is the largest exporter of chicken meat in the world.\n",
      "1:  Tokyo is the largest exporter of sake in the world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Osaka is the second largest and most famous of these sports.\n",
      "1:  Osaka is the capital of the prefecture.\n",
      "0:  Kyoto is the second largest manufacturing area of any country in the world.\n",
      "1:  Kyoto is the country s largest importer of rice  with over 4 000 exports.\n"
     ]
    }
   ],
   "source": [
    "#train eval 1 cycle\n",
    "#then create sample sentences\n",
    "train_epoch()\n",
    "eval_epoch()\n",
    "eval_keywords( keywords )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d4c0c068dbc3a0e32f455ee20b21324959693e968c5f5ea7f65133b266c1346"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
